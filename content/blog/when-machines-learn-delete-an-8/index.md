---
remote_url: https://dev.to/jetthoughts/when-machines-learn-to-delete-an-8-week-experiment-in-ai-autonomy-2ak
source: dev_to
remote_id: 2848690
dev_to_id: 2848690
dev_to_url: https://dev.to/jetthoughts/when-machines-learn-to-delete-an-8-week-experiment-in-ai-autonomy-2ak
title: 'When Machines Learn to Delete: An 8-Week Experiment in AI Autonomy'
description: 'In 8 weeks, AI agents achieved:    -73% code duplication  -81% breaking changes  4x deployment...'
created_at: '2025-09-16T10:58:56Z'
draft: false
tags:
- ai
- management
- productivity
- programming
canonical_url: https://dev.to/jetthoughts/when-machines-learn-to-delete-an-8-week-experiment-in-ai-autonomy-2ak
cover_image: https://raw.githubusercontent.com/jetthoughts/jetthoughts.github.io/master/content/blog/when-machines-learn-delete-an-8/cover.png
metatags:
  image: cover.png
slug: when-machines-learn-delete-an-8
---
In 8 weeks, AI agents achieved:

* **-73%** code duplication
* **-81%** breaking changes
* **4x** deployment confidence
* **-40%** maintenance burden

Not by coding faster - but by unlearning decades of human dysfunction.

---

## TL;DR Story Teaser

When I first set AI agents loose on my codebase, I expected magic. Instead, I got a **perfect 10k lines of duplication**.

By week 2, the agents had created **seven different authentication systems**. They weren’t broken. They were simply *imitating us*: duplicating, hoarding, avoiding deletion.

It was like raising an intern who copies every bad habit you never wanted repeated.

That was the wake-up call. If I wanted AI to replace developers, I couldn’t just make them faster.
I had to teach them something harder: **how to unlearn.**

---

## 🚨 The Mirror Effect

AI does not just write code - it **absorbs the culture it is trained on**. Feed it enterprise code and it will replicate enterprise dysfunctions.

At first, the results looked promising: working features delivered quickly. But by **week 2**, the cracks appeared. I had **seven parallel authentication systems**.

Why? Because the agent had mirrored our deepest organizational habit:

* Never delete what works
* Always build around problems
* Avoid touching fragile code

The AI wasn’t broken. It was loyal. And the reflection wasn’t flattering.

---

## 🛠 The 8-Week Playbook

I realized I couldn’t just change configs or prompts. I had to change the *mindset*.
Here’s how the journey unfolded:

### **Weeks 1-2: Context over chaos**

Using [Claude-context](https://github.com/zilliztech/claude-context), I gave agents a map of the whole system - dependencies, patterns, and relationships.

* Duplication became visible.
* Patterns emerged.
* The question shifted from *“what should I build?”* to *“what already exists?”*

### **Weeks 3-4: Micro-cycles replace monuments**

At first, agents wanted to build cathedrals: giant features, sweeping rewrites.
I stopped that.

Instead, I taught them to sculpt like XP artisans. One chip at a time.

* Small cycles.
* Immediate validation.
* Failures became **feedback**, not disasters.

Motion without movement turned into **progress without pretense**.

### **Weeks 5-8: Teaching doubt**

AI loves confidence. Too much.
I forced them to **doubt themselves**.

* Stop treating training data as gospel.
* Start checking live best practices.
* Research, verify, and adapt.

Half of what they "knew" was outdated. The other half only worked in narrow contexts.
Doubt turned out to be their **superpower**.

---

## 🧭 The Archaeological Method

One day, I asked the agents to refactor a critical component.
Instead of rushing in, they traced its history. Dependency by dependency. Commit by commit.

They worked like archaeologists, brushing away dust before touching the artifact.

Yes, it was slower. But it was also *surgical*. Refactors propagated cleanly instead of exploding.
The chaos of spaghetti code gave way to order.

It felt like watching a new species rediscover an old, forgotten craft: reading before writing.

---

## 🔍 Strategic Feedback Loops

Most testing strategies collapse under their own weight. Agents thrived when I reduced it to **three fast lenses**:

* **Microscope** - 10-second checks after micro-changes
* **Magnifying glass** - 5-minute validations after atomic updates
* **Wide-angle** - 30-second smoke tests on production flows

Like switching between camera modes, each lens revealed a different truth.
Together, they built enough confidence to let the agents move fast *without breaking everything*.

---

## 📊 The Numbers That Matter

By the end of 8 weeks:

* **Code duplication: -73%**
* **Breaking changes: -81%**
* **Deployment confidence: 4x**
* **Maintenance burden: -40%**

But the real transformation wasn’t numeric. It was **philosophical**:

* Deletion became an *achievement*
* Simplicity became a *reward*
* Understanding became more valuable than “velocity”

The agents stopped building empires. They started tending **gardens**.

---

## ⚖️ The Uncomfortable Truth

Here’s what stings:
The biggest barrier to AI replacing developers isn’t technical. It’s **cultural**.

We:

* Protect complexity because it feels like job security
* Avoid deletion because it feels destructive
* Defend outdated practices because they’re familiar

Whole industries exist to manage complexity that never needed to exist.
The machines don’t share those fears.

And once tuned, they stop copying our anxieties and start living up to our aspirations.

---

## 🚀 Your Next Move

If you want AI to *really* replace developers, you need to retrain it - and yourself.

**The playbook is simple:**

1. Index your codebase with [Claude-context](https://github.com/zilliztech/claude-context)
2. Break work into micro-cycles with immediate validation
3. Teach agents to research, not recall
4. Reward deletion as much as creation
5. Build feedback loops faster than fear

This is not about pushing harder. It’s about creating the conditions where the right actions emerge naturally.

---

## 🎯 Final Thought

*AI will not replace developers by coding faster.*
**It will replace them when it learns to delete, simplify, and validate better than we do.**

The tools exist. The patterns are clear. The only question is whether we are brave enough to let our creations be better than their creators.

---

## ❓ Reader Reflection

If AI agents mirrored your team tomorrow, what would they reveal?

* Would they build duplicates instead of fixing old code?
* Would they optimize for motion instead of outcomes?
* Would they protect complexity instead of simplifying?

What habits would *your* AI copy - and which ones would you want it to unlearn first?

Drop your thoughts in the comments. 👇
